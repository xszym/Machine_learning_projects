{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Riiid Kaggle Challenge\n\nBased on historical student answers model predicts how students will perform. \n\nI do data preprocessing and use LGBM with optuma to make a model.\n\nThis notebook archives 0.743 score.\n\n[You can read more about it here](https://www.kaggle.com/c/riiid-test-answer-prediction)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%%time\n\nimport sys\n\n# Regular Libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.image as mpimg\nimport gc\nfrom scipy.stats import pearsonr\n\nimport copy\nimport re\n\nimport warnings","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Datasets"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\n\ntrain = pd.read_csv(\n    '/kaggle/input/riiid-test-answer-prediction/train.csv',\n    usecols=[1, 2, 3, 4, 5, 7, 8, 9],\n    dtype={\n        'timestamp': 'int64',\n        'user_id': 'int32',\n        'content_id': 'int16',\n        'content_type_id': 'int8',\n        'task_container_id': 'int16',\n        'answered_correctly':'int8',\n        'prior_question_elapsed_time': 'float32',\n        'prior_question_had_explanation': 'boolean'\n    }\n)\n\n# # Read in data\n# dtypes = {\n#     \"row_id\": \"int64\",\n#     \"timestamp\": \"int64\",\n#     \"user_id\": \"int32\",\n#     \"content_id\": \"int16\",\n#     \"content_type_id\": \"boolean\",\n#     \"task_container_id\": \"int16\",\n#     \"user_answer\": \"int8\",\n#     \"answered_correctly\": \"int8\",\n#     \"prior_question_elapsed_time\": \"float32\", \n#     \"prior_question_had_explanation\": \"boolean\"\n# }\n\n# train = pd.read_hdf(\"../input/riiid-train-data-multiple-formats/riiid_train.h5\", \"riiid_train\")\n\n# # # Drop column as it doesn't give any information\n# train.drop(columns = [\"row_id\"], axis=1, inplace=True)\n# train.drop(columns = [\"user_answer\"], axis=1, inplace=True)\n\ntrain.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"questions = pd.read_csv('../input/riiid-test-answer-prediction/questions.csv')\n\nquestions.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lectures_df = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/lectures.csv')\nlectures_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features from Lectures data"},{"metadata":{"trusted":true},"cell_type":"code","source":"lectures_df['type_of'] = lectures_df['type_of'].replace('solving question', 'solving_question')\nlectures_df = pd.get_dummies(lectures_df, columns=['part', 'type_of'])\npart_lectures_columns = [column for column in lectures_df.columns if column.startswith('part')]\ntypes_of_lectures_columns = [column for column in lectures_df.columns if column.startswith('type_of_')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_lectures = train[train.content_type_id == True].merge(lectures_df, left_on='content_id', right_on='lecture_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_lecture_stats_part = train_lectures.groupby('user_id')[part_lectures_columns + types_of_lectures_columns].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in user_lecture_stats_part.columns:\n    bool_column = column + '_boolean'\n    user_lecture_stats_part[bool_column] = (user_lecture_stats_part[column] > 0).astype(int)\n    \nuser_lecture_stats_part.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\n\ndel train_lectures\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final features from Train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing lectures\ntrain = train[train.content_type_id == False].sort_values('timestamp').reset_index(drop = True)\n\nelapsed_mean = train.prior_question_elapsed_time.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Task container"},{"metadata":{"trusted":true},"cell_type":"code","source":"group1 = train.loc[(train.content_type_id == False), ['task_container_id', 'user_id']].groupby(['task_container_id']).agg(['count'])\ngroup1.columns = ['avg_questions']\ngroup2 = train.loc[(train.content_type_id == False), ['task_container_id', 'user_id']].groupby(['task_container_id']).agg(['nunique'])\ngroup2.columns = ['avg_questions']\ntask_container_id_features = group1 / group2 #group3\n\ndel group1, group2\ngc.collect()\n\ntask_container_id_features['avg_questions_seen'] = task_container_id_features.avg_questions.cumsum() # Cumulative sums, or running\ntask_container_id_features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Users"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# --- STUDENT ANSWERS ---\n# Group by student\ndef calculate_matrics_for_student(data):\n    # Calculate metrics\n    results = data.loc[data.content_type_id == False, ['user_id','answered_correctly']].\\\n                                groupby(['user_id']).\\\n                                agg({'answered_correctly': ['mean', 'count', 'std', 'median', 'var', 'skew']}).\\\n                                reset_index()\n    \n    results.columns = ['user_id', 'user_mean', 'user_count', 'user_std', \n                               'user_median', 'user_var', 'user_skew']\n\n    return results\n\n\nresults_user_final = calculate_matrics_for_student(train)\n# results_user_final['explanation_mean_user'] = train.loc[train.content_type_id == False, ['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\n# results_user_final\n# results_user_final.explanation_mean_user.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## question "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# --- CONTENT ID ANSWERS ---\n# Group by content\n# Calculate metrics \ndef calculate_metrics_for_content(data):\n    results = data[data['answered_correctly']!=-1].\\\n                                groupby('content_id').\\\n                                agg({'answered_correctly': ['mean', 'count', 'std', 'median', 'var', 'skew']}).\\\n                                reset_index()\n\n    results.columns = ['content_id', 'content_mean', 'content_count', 'content_std', \n                               'content_median', 'content_var', 'content_skew']\n    return results\n    \nresults_question_final = calculate_metrics_for_content(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features from question data"},{"metadata":{},"cell_type":"markdown","source":"## Tags"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nquestions['tags'] = questions['tags'].astype(str)\n\ntags = [x.split() for x in questions[questions.tags != \"nan\"].tags.values]\ntags = [item for elem in tags for item in elem]\ntags = set(tags)\nprint(f'There are {len(tags)} different tags')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# split tags\ntags_list = [x.split() for x in questions.tags.values]\nquestions['tags'] = tags_list\nquestions.head()\n\n# count right and wrong answers\ncorrect = train[train.answered_correctly != -1].groupby([\"content_id\", 'answered_correctly'], as_index=False).size()\ncorrect = correct.pivot(index= \"content_id\", columns='answered_correctly', values='size')\ncorrect.columns = ['Wrong', 'Right']\ncorrect = correct.fillna(0)\ncorrect[['Wrong', 'Right']] = correct[['Wrong', 'Right']].astype(int)\nquestions = questions.merge(correct, left_on = \"question_id\", right_on = \"content_id\", how = \"left\")\n\nquestions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntags = list(tags)\ntags_df = pd.DataFrame()\nfor x in range(len(tags)):\n    df = pd.DataFrame()\n    for y in range(len(questions)):\n        if (tags[x] in questions.tags.values[y]):\n            df = df.append(questions.iloc[y,:])\n\n    df = df.agg({'Wrong': ['sum'], 'Right': ['sum']})\n    df['tag'] = tags[x]\n#     df = df.set_index('tag')\n    tags_df = tags_df.append(df)\n\ntags_df['question_percent_correct'] = tags_df.Right/(tags_df.Right + tags_df.Wrong)\ntags_df = tags_df.sort_values(by = \"question_percent_correct\")\n\ntags_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntag_mean_procent = []\nfor question_tags in questions['tags']:\n    sum_of_tags = 0.0\n    values = 0.0\n    for tag_id in question_tags:\n        value_series = tags_df.loc[tags_df['tag'] == str(tag_id),'question_percent_correct']\n        if len(value_series) > 0:\n            values += 1\n            sum_of_tags = sum_of_tags + float(value_series[0])\n    if values > 0:\n        tag_mean_procent.append(sum_of_tags / values)\n    else:\n        tag_mean_procent.append(0.5)\n        \n        \nquestions['tags_mean_correct'] = tag_mean_procent\nquestions['question_percent_correct'] = questions.Right/(questions.Right + questions.Wrong)\nquestions['question_asked_time'] = (questions.Right + questions.Wrong)\nquestions.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\npart = questions.groupby('part').agg({'Wrong': ['sum'], 'Right': ['sum']}).reset_index()\npart['part_percent_correct'] = part.Right/(part.Right + part.Wrong)\npart = part.drop('Wrong', 1)\npart = part.drop('Right', 1)\npart.columns = ['part', 'part_percent_correct']\nquestions = questions.merge(part, how = 'left', on = 'part')\nquestions.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Questions finish "},{"metadata":{"trusted":true},"cell_type":"code","source":"questions = questions.drop('bundle_id', 1)\nquestions = questions.drop('correct_answer', 1)\n# questions = questions.drop('part', 1)\nquestions = questions.drop('tags', 1)\nquestions = questions.drop('Wrong', 1)\nquestions = questions.drop('Right', 1)\n\nquestions = questions.rename(columns={\"question_id\": \"content_id\"})\nquestions.to_parquet('question_features_data.parquet')\n\nquestions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['timestamp'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Spliting data"},{"metadata":{},"cell_type":"markdown","source":"## Validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"validation = train.groupby('user_id').tail(5)\ntrain = train[~train.index.isin(validation.index)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### calculate feature values for train without validation data"},{"metadata":{"trusted":true},"cell_type":"code","source":"results_user_validation = calculate_matrics_for_student(validation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.groupby('user_id').tail(18)\ntrain = train[~train.index.isin(X.index)]\nlen(X) + len(train) + len(validation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### calculate feature values for test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"results_user_X = calculate_matrics_for_student(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"del(train)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Question preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"content_mean = questions.question_percent_correct.mean()\n\n# set mean for question asked less then 3 times\nquestions.question_percent_correct = questions.question_percent_correct.mask((questions['question_asked_time'] < 3), content_mean)\n\nquestions.question_percent_correct = questions.question_percent_correct.mask((questions.question_percent_correct < .2) & (questions['question_asked_time'] < 21), .2)\nquestions.question_percent_correct = questions.question_percent_correct.mask((questions.question_percent_correct > .95) & (questions['question_asked_time'] < 21), .95)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Adding Features to dataset. (Marging)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def combine_features(data=None, \n                     user_features=None, \n                     question_features=questions, \n                     lectures_features=user_lecture_stats_part,\n                     task_container_id_features=task_container_id_features):\n    \n    # Add \"past\" information\n    data = data.merge(user_features, how = 'left', on = 'user_id')\n    data = data.merge(question_features, how = 'left', on = 'content_id')\n    data = data.merge(lectures_features, how = 'left', on = 'user_id')\n    data = data.merge(task_container_id_features, how=\"left\", left_on=['task_container_id'], right_index= True)\n    \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Adding features to validation dataset\n\nvalidation = combine_features(validation ,user_features=results_user_validation)\n\nX          = combine_features(X ,user_features=results_user_validation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lebel encoding "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n\ndef encode_categorical_values(data):\n    lb_make = LabelEncoder()\n    data.prior_question_had_explanation.fillna(False, inplace = True)\n    data[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(data[\"prior_question_had_explanation\"])\n    #     data[\"type_of_concept\"] = lb_make.fit_transform(data[\"type_of_concept\"])\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation = encode_categorical_values(validation)\nX = encode_categorical_values(X)\n\nX.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split data for X and Y"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = X['answered_correctly']\nX = X.drop(['answered_correctly'], axis=1)\n\nY_val = validation['answered_correctly']\nX_val = validation.drop(['answered_correctly'], axis=1)\n\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Filter columns. Get only one with features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#user_id content_id\ttask_container_id prior_question_had_explanation \n\nfeatures_columns_names = ['prior_question_elapsed_time', \n                          'user_mean', 'user_count', 'user_std', 'user_median', 'user_var', 'user_skew', 'part',\n                          'tags_mean_correct', 'question_percent_correct', 'question_asked_time', 'part_percent_correct',\n                          'part_1', 'part_2', 'part_3', 'part_4', 'part_5', 'part_6', 'part_7',\n                          'type_of_intention', 'type_of_solving_question', 'type_of_starter',\n                          'part_1_boolean', 'part_2_boolean', 'part_3_boolean', 'part_4_boolean', 'part_5_boolean', 'part_6_boolean', 'part_7_boolean',\n                          'type_of_concept_boolean', 'type_of_intention_boolean',\n                          'type_of_solving_question_boolean', 'type_of_starter_boolean',\n                          'avg_questions_seen', 'prior_question_had_explanation_enc'\n                         ]\n\n\nX = X[features_columns_names]\nX_val = X_val[features_columns_names]\n\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Remove nans"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_nans(data):\n    data['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\n    \n    data['user_mean'].fillna(0.65,  inplace=True)\n    data['user_count'].fillna(0,  inplace=True)\n    data['user_std'].fillna(0.42,  inplace=True)\n    data['user_median'].fillna(1,  inplace=True)\n    data['user_var'].fillna(0.21,  inplace=True)\n    data['user_skew'].fillna(-0.16,  inplace=True)\n    \n    data['tags_mean_correct'].fillna(0.65,  inplace=True)\n    data['question_percent_correct'].fillna(content_mean,  inplace=True)\n    data['question_asked_time'].fillna(0,  inplace=True)\n    data['part_percent_correct'].fillna(0.65,  inplace=True)\n\n    data['part_1'].fillna(0, inplace = True)\n    data['part_2'].fillna(0, inplace = True)\n    data['part_3'].fillna(0, inplace = True)\n    data['part_4'].fillna(0, inplace = True)\n    data['part_5'].fillna(0, inplace = True)\n    data['part_6'].fillna(0, inplace = True)\n    data['part_7'].fillna(0, inplace = True)\n    \n    data['type_of_intention'].fillna(0, inplace = True)\n    data['type_of_solving_question'].fillna(0, inplace = True)\n    data['type_of_starter'].fillna(0, inplace = True)\n\n    data['part_1_boolean'].fillna(0, inplace = True)\n    data['part_2_boolean'].fillna(0, inplace = True)\n    data['part_3_boolean'].fillna(0, inplace = True)\n    data['part_4_boolean'].fillna(0, inplace = True)\n    data['part_5_boolean'].fillna(0, inplace = True)\n    data['part_6_boolean'].fillna(0, inplace = True)\n    data['part_7_boolean'].fillna(0, inplace = True)\n    \n    # data['type_of_concept'].fillna(0, inplace = True)\n\n    data['type_of_concept_boolean'].fillna(0, inplace = True)\n    data['type_of_intention_boolean'].fillna(0, inplace = True)\n    data['type_of_solving_question_boolean'].fillna(0, inplace = True)\n    data['type_of_starter_boolean'].fillna(0, inplace = True)\n        \n    data['part'].fillna(4, inplace = True)\n    data['avg_questions_seen'].fillna(1, inplace = True)\n    data['prior_question_had_explanation_enc'].fillna(0, inplace = True)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = remove_nans(X)\nX_val = remove_nans(X_val)\n\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#maybe I will ues it for other models\ndef scale_data(data=None, columns_to_scale=None):\n    scaled_features = data\n    \n    features = scaled_features[columns_to_scale]\n    scaler = StandardScaler().fit(features.values)\n    features = scaler.transform(features.values)\n    \n    scaled_features[columns_to_scale] = features\n\n    return scaled_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nparams = {\n    'num_leaves': 31, \n    'n_estimators': 200, \n    'max_depth': 8, \n    'min_child_samples': 356, \n    'learning_rate': 0.2982483634778906, \n    'min_data_in_leaf': 82, \n    'bagging_fraction': 0.6545628633239445, \n    'feature_fraction': 0.9164482379289846,\n    'random_state': 666\n}\n\nfull_model = LGBMClassifier(**params)\nfull_model.fit(X, Y)\n\npreds = full_model.predict_proba(X_val)[:,1]\nprint('LGB roc auc', roc_auc_score(Y_val, preds))\n\nfull_xgb = XGBClassifier(random_state=666)\nfull_xgb.fit(X, Y)\n\npreds = full_xgb.predict_proba(X_val)[:,1]\nprint('XGB roc auc', roc_auc_score(Y_val, preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import optuna\nfrom optuna.samplers import TPESampler\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n\nrfe = RFE(estimator=DecisionTreeClassifier(random_state=666), n_features_to_select=14)\nrfe.fit(X, Y)\nX = rfe.transform(X)\nX_val = rfe.transform(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n\nsampler = TPESampler(seed=666)\n\ndef create_model(trial):\n    num_leaves = trial.suggest_int(\"num_leaves\", 2, 31)\n    n_estimators = trial.suggest_int(\"n_estimators\", 20, 300)\n    max_depth = trial.suggest_int('max_depth', 3, 9)\n    min_child_samples = trial.suggest_int('min_child_samples', 100, 1200)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0001, 0.99)\n    min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 5, 90)\n    bagging_fraction = trial.suggest_uniform('bagging_fraction', 0.0001, 1.0)\n    feature_fraction = trial.suggest_uniform('feature_fraction', 0.0001, 1.0)\n    model = LGBMClassifier(\n        num_leaves=num_leaves,\n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        min_child_samples=min_child_samples, \n        min_data_in_leaf=min_data_in_leaf,\n        learning_rate=learning_rate,\n        feature_fraction=feature_fraction,\n        random_state=666\n        )\n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(X, Y)\n    preds = model.predict_proba(X_val)[:,1]\n    score = roc_auc_score(Y_val, preds)\n    return score\n\n# run optuna\nstudy = optuna.create_study(direction=\"maximize\", sampler=sampler)\nstudy.optimize(objective, n_trials=350)\nparams = study.best_params\nparams['random_state'] = 666\n\n# params = {\n#     'num_leaves': 28, \n#     'n_estimators': 295, \n#     'max_depth': 8, \n#     'min_child_samples': 1178, \n#     'learning_rate': 0.2379173491475032, \n#     'min_data_in_leaf': 35, \n#     'bagging_fraction': 0.8389723511600549, \n#     'feature_fraction': 0.9606189400533491,\n#     'random_state': 666\n# }\n\nmodel = LGBMClassifier(**params)\nmodel.fit(X, Y)\n\npreds = model.predict_proba(X_val)[:,1]\nroc_auc_score(Y_val, preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n\nX = pd.DataFrame(X)\nX_val = pd.DataFrame(X_val)\n\nY = pd.DataFrame(Y)\nY_val = pd.DataFrame(Y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n\nmodels = []\npreds = []\nfor n, (tr, te) in enumerate(KFold(n_splits=5, random_state=666, shuffle=True).split(Y)):\n    print(f'Fold {n}')\n    model = LGBMClassifier(**params)\n    model.fit(X.values[tr], Y.values[tr])\n    \n    pred = model.predict_proba(X_val)[:, 1]\n    preds.append(pred)\n    print('Fold roc auc:', roc_auc_score(Y.values[te], model.predict_proba(X.values[te])[:, 1])) \n    models.append(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n\npredictions = preds[0]\nfor i in range(1, 5):\n    predictions += preds[i]\npredictions /= 5\n\nprint('ROC AUC', roc_auc_score(Y_val, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import library and create environment\nimport riiideducation\nenv = riiideducation.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iter_test = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (test_df, sample_prediction_df) in iter_test:\n    test_df['task_container_id'] = test_df.task_container_id.mask(test_df.task_container_id > 9999, 9999)\n    \n    test_df = combine_features(test_df ,user_features=results_user_final)\n    test_df = encode_categorical_values(test_df)\n    test_df = remove_nans(test_df)\n    \n    full_preds = full_model.predict_proba(test_df[features_columns_names])[:, 1]\n    \n    full_preds_xgb = full_xgb.predict_proba(test_df[features_columns_names])[:, 1]\n    \n    X_test = rfe.transform(test_df[features_columns_names])\n    \n    preds = [model.predict_proba(X_test)[:,1] for model in models]\n    \n    predictions = preds[0]\n    for i in range(1, 5):\n        predictions += preds[i]\n    predictions /= 5\n    \n    test_df['answered_correctly'] =  predictions * 0.6 + full_preds * 0.2 + full_preds_xgb * 0.2\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}